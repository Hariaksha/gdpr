{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e3557a41-3ef8-4956-9f3a-060a64d5fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a23a55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data into new dataframe\n",
    "df = pd.read_csv(\"/Users/hariaksha/Documents/GitHub/gdpr/data/welle120_aufbereitet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a3d07af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in dataframe: 1352\n",
      "       ums    nach    pers    umse   nache   perse  pdwichtig  umheute  \\\n",
      "5   runter  runter  gleich  runter  runter  runter        3.0      4.0   \n",
      "6     hoch    hoch    hoch  gleich  gleich    hoch        5.0      5.0   \n",
      "7   gleich  gleich  gleich  gleich  gleich  gleich        5.0      4.0   \n",
      "11  gleich    hoch  gleich    hoch    hoch  gleich        2.0      5.0   \n",
      "13  runter  runter  runter  runter  runter  gleich        5.0      4.0   \n",
      "\n",
      "    aspekte  aufwand  ...  br_zahl               br08   bges      gk9    gk4  \\\n",
      "5       2.0      2.0  ...      5.0          UBeratung  395.0  250-499  >=100   \n",
      "6       2.0      3.0  ...      1.0  IKT-Dienstleister   46.0    20-49  20-99   \n",
      "7       2.0      3.0  ...      2.0          ReSteuWip   23.0    20-49  20-99   \n",
      "11      2.0      3.0  ...      1.0  IKT-Dienstleister   31.0    20-49  20-99   \n",
      "13      3.0      3.0  ...      1.0             techDL   50.0    50-99  20-99   \n",
      "\n",
      "       aweight  online      vg                            br04  \\\n",
      "5    11.625000       1  Infowi  Wissensintensive Dienstleister   \n",
      "6   127.187500       1  Infowi                     IKT-Branche   \n",
      "7    86.622222       1  Infowi  Wissensintensive Dienstleister   \n",
      "11  127.187500       1  Infowi                     IKT-Branche   \n",
      "13   83.436364       1  Infowi  Wissensintensive Dienstleister   \n",
      "\n",
      "                              br07  \n",
      "5   Wissensintensive Dienstleister  \n",
      "6                      IKT-Branche  \n",
      "7   Wissensintensive Dienstleister  \n",
      "11                     IKT-Branche  \n",
      "13  Wissensintensive Dienstleister  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "         ums    nach    pers    umse   nache   perse  pdwichtig  umheute  \\\n",
      "1342  runter  runter  gleich    hoch  gleich  gleich        5.0      4.0   \n",
      "1346  runter  gleich  runter  gleich  gleich  gleich        5.0      4.0   \n",
      "1347  gleich    hoch  runter  gleich  gleich  gleich        4.0      4.0   \n",
      "1349    hoch    hoch  gleich    hoch    hoch  gleich        5.0      5.0   \n",
      "1350  gleich    hoch  gleich  gleich  gleich  gleich        3.0      5.0   \n",
      "\n",
      "      aspekte  aufwand  ...  br_zahl           br08   bges      gk9    gk4  \\\n",
      "1342      2.0      2.0  ...      1.0   Maschinenbau    7.0      5-9   5-19   \n",
      "1346      3.0      4.0  ...      1.0        Werbung   34.0    20-49  20-99   \n",
      "1347      3.0      3.0  ...      2.0   Maschinenbau  220.0  100-249  >=100   \n",
      "1349      3.0      4.0  ...      3.0  Chemie/Pharma    5.0      5-9   5-19   \n",
      "1350      2.0      2.0  ...      2.0  Medienbranche    7.0      5-9   5-19   \n",
      "\n",
      "         aweight  online      vg                            br04  \\\n",
      "1342  155.200000       1      VG                              VG   \n",
      "1346   39.933333       1  Infowi  Wissensintensive Dienstleister   \n",
      "1347   39.745098       1      VG                              VG   \n",
      "1349   25.512195       1      VG                              VG   \n",
      "1350  130.291667       1  Infowi             Mediendienstleister   \n",
      "\n",
      "                                br07  \n",
      "1342                    Maschinenbau  \n",
      "1346  Wissensintensive Dienstleister  \n",
      "1347                    Maschinenbau  \n",
      "1349                   Chemie/Pharma  \n",
      "1350             Mediendienstleister  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "Total number of rows in cleaned dataframe: 637\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of rows in dataframe: {len(df)}\")\n",
    "\n",
    "# Filter out rows with empty values in specific columns\n",
    "columns = ['erf_speich', 'si_speich', 'erf_intern', 'si_intern', 'erf_extunt', 'si_extunt', 'erf_forsch', \n",
    "                  'si_forsch', 'a_rsicher', 'pdwichtig', 'a_nachteil', 'a_gefahr', 'a_kompliz', 'a_vorteil', \n",
    "                  'a_vertrau', 'a_inno', 'a_kost', 'a_berat', 'a_proz', 'a_stand', 'a_ki', 'a_forsch', \n",
    "                  'aufwand', 'datengm', 'umheute', 'nach', 'ums', 'pers', 'nache', 'umse', 'perse']\n",
    "df_clean = df.dropna(subset=columns)\n",
    "# df_clean = df_clean.drop([\"beh_grund\", \"online\", \"vg\", 'br08', 'bges', 'gk9', 'gk4', 'aweight', 'br04', 'br07'], axis=1)\n",
    "\n",
    "print(df_clean.head())\n",
    "print(df_clean.tail())\n",
    "print(f\"Total number of rows in cleaned dataframe: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb9a74",
   "metadata": {},
   "source": [
    "**Descriptive Statistics**\\\n",
    "In the following code block, we find the following descriptive statistics to describe the sample:\n",
    "* standard deviation\n",
    "* sample size\n",
    "* minimum\n",
    "* maximum\n",
    "* median\n",
    "\n",
    "We also find the following inferential statistics to make inferences about the general population:\n",
    "* confidence interval upper and lower bounds\n",
    "    * Note that a 95% confidence interval indicates that there is a 95% chance that the true population mean is in that interval. This does not mean that 95% of the population is in this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_of_interest = 'a_inno'\n",
    "# Display basic statistics for the column of interest\n",
    "print(f\"Descriptive statistics for {column_of_interest}:\")\n",
    "print(df_clean[column_of_interest].describe())\n",
    "print(\"Skew:\\t\", df_clean[column_of_interest].skew())\n",
    "\n",
    "# Display confidence interval for the column of interest\n",
    "print(stats.norm.interval(confidence = 0.95, loc = df_clean[column_of_interest].mean(), scale = (df_clean[column_of_interest].std()/np.sqrt(df_clean[column_of_interest].count()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90398fd5",
   "metadata": {},
   "source": [
    "**Hypothesis Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_of_interest = 'a_inno'\n",
    "column_of_interest2 = 'a_kost'\n",
    "\n",
    "# Perform one-sample z-test\n",
    "mean_null_hypothesis = 3  # Hypothesized population mean\n",
    "result = ztest(df_clean[column_of_interest], value=mean_null_hypothesis)\n",
    "print(f\"The test statistic (z-score) and p-value for the one-sample z-test on {column_of_interest} with null hypothesis mean {mean_null_hypothesis} are: {result[0]} and {result[1]}\")\n",
    "\n",
    "print(f\"Means of {column_of_interest} and {column_of_interest2}: {df_clean[column_of_interest].mean()} and {df_clean[column_of_interest2].mean()}\")\n",
    "\n",
    "# Perform two-sample z-test\n",
    "mean_null_hypothesis = 3  # Hypothesized population mean\n",
    "result = ztest(df_clean[column_of_interest], df_clean[column_of_interest2], value=mean_null_hypothesis)\n",
    "print(f\"The test statistic (z-score) and p-value for the two-sample z-test on {column_of_interest} and {column_of_interest2} are: {result[0]} and {result[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7efcfd",
   "metadata": {},
   "source": [
    "**Create Uniform or Normal Random Simulation Sampling Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151137c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_of_interest = 'a_inno'\n",
    "sample = np.random.uniform(low=1.0, high=5.0, size=1000)\n",
    "sample = np.random.normal(loc=df_clean[column_of_interest].mean(), scale=df_clean[column_of_interest].std(), size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts in table and percentage format\n",
    "column_of_interest = 'a_inno'\n",
    "print(df_clean[column_of_interest].value_counts())\n",
    "print(df_clean[column_of_interest].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Get average response for column\n",
    "average_response = df_clean[column_of_interest].mean()\n",
    "print(f\"Average response for {column_of_interest}: {average_response}\")\n",
    "\n",
    "# Create a bar plot for the one column\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df_clean, x=column_of_interest, order=df_clean[column_of_interest].value_counts().index)\n",
    "plt.title('Distribution of ' + column_of_interest)\n",
    "plt.xlabel(column_of_interest)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5bfe8-58e9-47c1-9062-daaa9270a4dd",
   "metadata": {},
   "source": [
    "**Recoding Variables**\n",
    "\n",
    "We need to recode most variables to binary variables (0/1).\n",
    "* The variables ums, umse, nach, nache, pers, and perse each have responses of increasing (), same (gleich), or decreasing (gefallen); they will be used to create new variables such as umsgleich and nachegesunken, which are respectively 1 if the ums response is 'gleich' or if the nache response is 'gesunken'.\n",
    "* The other variables, for example kund_dat, all either have responses from one to four or one to five; they wll be used to create new variables such as kund_dat2, kund_dat3, kund_dat4 and kund_dat5, kund_dat23, and kund_dat45.\n",
    "* Note that the responses of 'increasing' and '1' for all variables have not been encoded. This is because these variables wll be included or carried in the intercept in any regression, and they therefore do not need to be explicitly encoded.\n",
    "\n",
    "This recoding will allow us to do multivariable binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c31851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the erf variables to 0s and 1s instead of 1s and 2s\n",
    "# recode to 0/1: 1 = consent required, 0 = not required\n",
    "for col in ['erf_extunt', 'erf_intern', 'erf_forsch', 'erf_speich']:\n",
    "    df_clean[col] = df_clean[col] == 1\n",
    "\n",
    "# recode variables / create binary data variables for high certainty for 4 situational questions (si_speich, si_intern, si_extunt, si_forsch)\n",
    "new_cols = pd.DataFrame({\n",
    "    'si_speich_high': (df_clean['si_speich'] > 3),\n",
    "    'si_intern_high': (df_clean['si_intern'] > 3),\n",
    "    'si_extunt_high': (df_clean['si_extunt'] > 3),\n",
    "    'si_forsch_high': (df_clean['si_forsch'] > 3)\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "new_cols = pd.DataFrame({\n",
    "    'sicher_speich_certainty_and_yes': (df_clean['si_speich_high'] == 1) & (df_clean['erf_speich'] == 1),\n",
    "    'sicher_intern_certainty_and_yes': (df_clean['si_intern_high'] == 1) & (df_clean['erf_intern'] == 1),\n",
    "    'sicher_extunt_certainty_and_yes': (df_clean['si_extunt_high'] == 1) & (df_clean['erf_extunt'] == 1),\n",
    "    'sicher_forsch_certainty_and_yes': (df_clean['si_forsch_high'] == 1) & (df_clean['erf_forsch'] == 1)\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# recode the agreement questions\n",
    "new_cols = pd.DataFrame({\n",
    "    'a_positiv_zustimmung': (df_clean['a_positiv'] > 3),\n",
    "    'a_rsicher_zustimmung': (df_clean['a_rsicher'] > 3),\n",
    "    'a_gefahr_zustimmung': (df_clean['a_gefahr'] > 3),\n",
    "    'a_aufwand_zustimmung': (df_clean['a_aufwand'] > 3),\n",
    "    'a_kompliz_zustimmung': (df_clean['a_kompliz'] > 3),\n",
    "    'a_vorteil_zustimmung': (df_clean['a_vorteil'] > 3),\n",
    "    'a_vertrau_zustimmung': (df_clean['a_vertrau'] > 3),\n",
    "    'a_inno_zustimmung': (df_clean['a_inno'] > 3),\n",
    "    'a_kost_zustimmung': (df_clean['a_kost'] > 3),\n",
    "    'a_berat_zustimmung': (df_clean['a_berat'] > 3),\n",
    "    'a_proz_zustimmung': (df_clean['a_proz'] > 3),\n",
    "    'a_stand_zustimmung': (df_clean['a_stand'] > 3),\n",
    "    'a_ki_zustimmung': (df_clean['a_ki'] > 3),\n",
    "    'a_forsch_zustimmung': (df_clean['a_forsch'] > 3),\n",
    "    'a_nachteil_zustimmung': (df_clean['a_nachteil'] > 3)\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# create binary variable to see if firm has high certainty in all scenarios\n",
    "df_clean['always_high_certainty'] = (df_clean['si_speich'] > 3) & (df_clean['si_intern'] > 3) & (df_clean['si_extunt'] > 3) & (df_clean['si_forsch'] > 3)\n",
    "\n",
    "# create binary variable for if firm has high certainty (4 or 5) on average\n",
    "df_clean['avg_high_certainty'] = df_clean['si_speich'] + df_clean['si_intern'] + df_clean['si_extunt'] + df_clean['si_forsch'] >= 16\n",
    "\n",
    "# use dummy coding to create new binary variables for pdwichtig\n",
    "new_cols = pd.DataFrame({\n",
    "    'pdwichtig2': (df_clean['pdwichtig'] == 2),\n",
    "    'pdwichtig3': (df_clean['pdwichtig'] == 3),\n",
    "    'pdwichtig4': (df_clean['pdwichtig'] == 4),\n",
    "    'pdwichtig5': (df_clean['pdwichtig'] == 5),\n",
    "    'pdwichtig23': (df_clean['pdwichtig'].isin([2, 3])),\n",
    "    'pdwichtig45': (df_clean['pdwichtig'].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for aufwand\n",
    "new_cols = pd.DataFrame({\n",
    "    'aufwand2': (df_clean['aufwand'] == 2),\n",
    "    'aufwand3': (df_clean['aufwand'] == 3),\n",
    "    'aufwand4': (df_clean['aufwand'] == 4),\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for kund_dat\n",
    "new_cols = pd.DataFrame({\n",
    "    'kund_dat2': (df_clean['kund_dat'] == 2),\n",
    "    'kund_dat3': (df_clean['kund_dat'] == 3),\n",
    "    'kund_dat4': (df_clean['kund_dat'] == 4),\n",
    "    'kund_dat5': (df_clean['kund_dat'] == 5),\n",
    "    'kund_dat23': (df_clean['kund_dat'].isin([2, 3])),\n",
    "    'kund_dat45': (df_clean['kund_dat'].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for datengm\n",
    "column_of_interest = 'datengm'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for umheute\n",
    "column_of_interest = 'umheute'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for unsicher\n",
    "df_clean['unsicher_high'] = df_clean['unsicher'].isin([3, 4])\n",
    "df_clean['unsicher_low'] = df_clean['unsicher'].isin([1, 2])\n",
    "\n",
    "# use dummy coding to create new binary variables for ums\n",
    "df_clean['umsgleich'] = df_clean['ums'] == \"gleich\"\n",
    "df_clean['umsgesunken'] = df_clean['ums'] == \"runter\"\n",
    "\n",
    "# use dummy coding to create new binary variables for nach\n",
    "df_clean['nachgleich'] = df_clean['nach'] == \"gleich\"\n",
    "df_clean['nachgesunken'] = df_clean['nach'] == \"runter\"\n",
    "\n",
    "# use dummy coding to create new binary variables for pers\n",
    "df_clean['persgleich'] = df_clean['pers'] == \"gleich\"\n",
    "df_clean['persgesunken'] = df_clean['pers'] == \"runter\"\n",
    "\n",
    "# use dummy coding to create new binary variables for umse\n",
    "df_clean['umsegleich'] = df_clean['umse'] == \"gleich\"\n",
    "df_clean['umsegesunken'] = df_clean['umse'] == \"runter\"\n",
    "\n",
    "# use dummy coding to create new binary variables for nache\n",
    "df_clean['nachegleich'] = df_clean['nache'] == \"gleich\"\n",
    "df_clean['nachegesunken'] = df_clean['nache'] == \"runter\"\n",
    "\n",
    "# use dummy coding to create new binary variables for perse\n",
    "df_clean['persegleich'] = df_clean['perse'] == \"gleich\"\n",
    "df_clean['persegesunken'] = df_clean['perse'] == \"runter\"\n",
    "\n",
    "# use dummy coding to create new binary variables for a_positiv\n",
    "column_of_interest = 'a_positiv'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_rsicher\n",
    "column_of_interest = 'a_rsicher'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_gefahr\n",
    "column_of_interest = 'a_gefahr'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_aufwand\n",
    "column_of_interest = 'a_aufwand'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_kompliz\n",
    "column_of_interest = 'a_kompliz'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_vorteil\n",
    "column_of_interest = 'a_vorteil'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_vertrau\n",
    "column_of_interest = 'a_vertrau'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_inno\n",
    "column_of_interest = 'a_inno'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_kost\n",
    "column_of_interest = 'a_kost'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_berat\n",
    "column_of_interest = 'a_berat'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_proz\n",
    "column_of_interest = 'a_proz'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_stand\n",
    "column_of_interest = 'a_stand'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_ki\n",
    "column_of_interest = 'a_ki'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_forsch\n",
    "column_of_interest = 'a_forsch'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# use dummy coding to create new binary variables for a_nachteil\n",
    "column_of_interest = 'a_nachteil'\n",
    "new_cols = pd.DataFrame({\n",
    "    column_of_interest + \"2\": (df_clean[column_of_interest] == 2),\n",
    "    column_of_interest + \"3\": (df_clean[column_of_interest] == 3),\n",
    "    column_of_interest + \"4\": (df_clean[column_of_interest] == 4),\n",
    "    column_of_interest + \"5\": (df_clean[column_of_interest] == 5),\n",
    "    column_of_interest + \"23\": (df_clean[column_of_interest].isin([2, 3])),\n",
    "    column_of_interest + \"45\": (df_clean[column_of_interest].isin([4, 5]))\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# create burden index\n",
    "df_clean['burden'] = df_clean[['a_aufwand', 'a_kompliz', 'a_kost', 'a_berat']].mean(axis=1)\n",
    "df_clean['burden_high'] = df_clean['burden'] >= 4\n",
    "\n",
    "# create opportunity index\n",
    "df_clean['opportunity'] = df_clean[['a_positiv', 'a_rsicher', 'a_vorteil', 'a_vertrau', 'a_proz', 'a_stand']].mean(axis=1)\n",
    "df_clean['opportunity_high'] = df_clean['opportunity'] >= 4\n",
    "\n",
    "# create dummy variable to see if firm still shares data or has ever shared data\n",
    "new_cols = pd.DataFrame({\n",
    "    \"ds_unter_aktuell\": df_clean['ds_unter'] == 1,\n",
    "    \"ds_forsch_aktuell\": df_clean['ds_forsch'] == 1,\n",
    "    \"ds_unter_ever\": df_clean['ds_unter'].isin([1, 2]),\n",
    "    \"ds_forsch_ever\": df_clean['ds_forsch'].isin([1, 2])\n",
    "})\n",
    "df_clean = pd.concat([df_clean, new_cols], axis=1)\n",
    "\n",
    "# build dummies for size (gk4) and sector (br08)\n",
    "size_dummies = pd.get_dummies(df_clean['gk4'], prefix='gk9', drop_first=True)\n",
    "sector_dummies = pd.get_dummies(df_clean['br08'], prefix='br08', drop_first=True)\n",
    "df_clean = pd.concat([df_clean, size_dummies, sector_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows and cols\", df_clean.shape)\n",
    "print(df_clean.head(3))\n",
    "# print(df_clean.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e260e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Squared Test\n",
    "contingency_table = pd.crosstab(df_clean['sicher_speich_binary'], df_clean['erf_speich'])\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(\"Chi-squared:\", chi2)\n",
    "print(\"p-value:\", p)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(\"Expected frequencies:\\n\", expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a2483",
   "metadata": {},
   "source": [
    "**Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = 'si_speich'\n",
    "col_of_interest = 'erf_speich'\n",
    "\n",
    "print(df_clean['si_speich'].value_counts())\n",
    "\n",
    "grouped = df_clean.groupby(segmenter)\n",
    "grouped[col_of_interest].mean().plot(kind='bar', figsize=(8, 6))\n",
    "plt.title(f'Mean {col_of_interest} by {segmenter}')\n",
    "df_clean.pivot_table(index=segmenter, values=col_of_interest, aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fe422",
   "metadata": {},
   "source": [
    "**See Correlations**\n",
    "\n",
    "Useful to identify most predictive features or independent variables before creating any regression model. This is the Pearson correlation coefficient, which makes it only useful for linear association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatterplots can be useful to make scatter plots\n",
    "columns = ['a_positiv', 'a_rsicher', 'a_gefahr']\n",
    "\n",
    "sns.pairplot(df_clean[columns]) # see all possible scatter plots\n",
    "\n",
    "df[columns].corr() # see all correlation coefficients for cols that you want\n",
    "\n",
    "print(df[columns].head()) \n",
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "columns = ['erf_speich', 'si_speich', 'erf_intern', 'si_intern', 'erf_extunt', 'si_extunt', 'erf_forsch', 'si_forsch', \n",
    "                  'a_rsicher', 'pdwichtig', 'a_nachteil', 'a_gefahr', 'a_kompliz', 'a_vorteil', 'a_vertrau', 'a_inno', \n",
    "                  'a_kost', 'a_berat', 'a_proz', 'a_stand', 'a_ki', 'a_forsch', 'aufwand', 'datengm', 'umheute']\n",
    "plt.title(\"Correlation Matrix Dataset\")\n",
    "sns.heatmap(df[columns].corr(), color=\"k\", annot=True, cmap=\"YlGnBu\") # correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a25b96",
   "metadata": {},
   "source": [
    "**Create Logistic Regression Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86920c",
   "metadata": {},
   "source": [
    "This is the first logistic regression model that I made. The response variable is erf_extunt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f55dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.303004\n",
      "         Iterations: 35\n",
      "Logit Model\n",
      "                            Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:             erf_extunt   No. Observations:                  445\n",
      "Model:                          Logit   Df Residuals:                      431\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Sat, 21 Feb 2026   Pseudo R-squ.:                 0.07492\n",
      "Time:                        16:35:00   Log-Likelihood:                -134.84\n",
      "converged:                      False   LL-Null:                       -145.76\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.05787\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          21.6094   1.06e+04      0.002      0.998   -2.07e+04    2.08e+04\n",
      "si_extunt       0.4316      0.147      2.942      0.003       0.144       0.719\n",
      "pdwichtig23    -1.2974      0.851     -1.524      0.128      -2.966       0.371\n",
      "pdwichtig45    -0.9096      0.881     -1.033      0.302      -2.636       0.817\n",
      "kund_dat23    -20.2044   1.06e+04     -0.002      0.998   -2.08e+04    2.07e+04\n",
      "kund_dat45    -19.6321   1.06e+04     -0.002      0.999   -2.08e+04    2.07e+04\n",
      "datengm23      -0.1730      0.865     -0.200      0.842      -1.868       1.523\n",
      "datengm45      -0.7611      0.885     -0.860      0.390      -2.496       0.974\n",
      "a_positiv23    -0.4279      0.388     -1.102      0.270      -1.189       0.333\n",
      "a_positiv45    -0.1631      0.871     -0.187      0.851      -1.870       1.544\n",
      "a_rsicher23     0.4720      0.437      1.080      0.280      -0.385       1.329\n",
      "a_rsicher45     0.1057      0.527      0.200      0.841      -0.928       1.139\n",
      "a_gefahr23      0.2350      0.365      0.643      0.520      -0.481       0.951\n",
      "a_gefahr45     -0.2161      0.557     -0.388      0.698      -1.308       0.876\n",
      "===============================================================================\n",
      "Confusion matrix:\n",
      " [[  0  20]\n",
      " [  0 172]]\n",
      "Accuracy: 0.8958333333333334\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        20\n",
      "           1       0.90      1.00      0.95       172\n",
      "\n",
      "    accuracy                           0.90       192\n",
      "   macro avg       0.45      0.50      0.47       192\n",
      "weighted avg       0.80      0.90      0.85       192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# X: dataframe of 0/1 features, y: 0/1 target\n",
    "X = df_clean[['si_extunt', 'pdwichtig23', 'pdwichtig45', 'kund_dat23', 'kund_dat45', 'datengm23', 'datengm45', 'a_positiv23', 'a_positiv45', 'a_rsicher23', 'a_rsicher45', 'a_gefahr23', 'a_gefahr45']].astype(int) # df_binary_features\n",
    "y = df_clean['erf_extunt'].astype(int) # df_binary_target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# statsmodels on training data only\n",
    "X_train_sm = sm.add_constant(X_train) # convert True/False â†’ 1/0 because that is what the statsmodel ML trainer predicts\n",
    "logit_model = sm.Logit(y_train, X_train_sm)\n",
    "result = logit_model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# evaluate on test data\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "y_pred_prob = result.predict(X_test_sm)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# confusion matrix, accuracy, and full report\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368366b",
   "metadata": {},
   "source": [
    "**Test Logistic Regression Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2feb6",
   "metadata": {},
   "source": [
    "**Principal Component Analysis** \n",
    "\n",
    "for before trying the sklearn logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c662c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'erf_extunt'\n",
    "X = df_clean.drop(target, axis=1)\n",
    "y = df_clean[target].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = PCA(n_components=4)\n",
    "model.fit(X_iris)          \n",
    "X_2D = model.transform(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "300e0221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const                     float64\n",
      "unsicher_high               int64\n",
      "burden_high                 int64\n",
      "opportunity_high            int64\n",
      "pdwichtig                 float64\n",
      "datengm                   float64\n",
      "ds_unter_aktuell             bool\n",
      "ds_forsch_aktuell            bool\n",
      "kund_dat                  float64\n",
      "gk9_100-249                  bool\n",
      "gk9_20-49                    bool\n",
      "gk9_250-499                  bool\n",
      "gk9_5-9                      bool\n",
      "gk9_50-99                    bool\n",
      "gk9_500-999                  bool\n",
      "gk9_>=1000                   bool\n",
      "br08_Fahrzeugbau             bool\n",
      "br08_FuE                     bool\n",
      "br08_IKT-Dienstleister       bool\n",
      "br08_IKT-Hardware            bool\n",
      "br08_Maschinenbau            bool\n",
      "br08_Medienbranche           bool\n",
      "br08_ReSteuWip               bool\n",
      "br08_Sonstiges VG            bool\n",
      "br08_UBeratung               bool\n",
      "br08_Werbung                 bool\n",
      "br08_sonstigeDL              bool\n",
      "br08_techDL                  bool\n",
      "dtype: object\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(X.dtypes)\n",
    "print(weights.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a94bf",
   "metadata": {},
   "source": [
    "**Linear Probability Model and Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bd7673e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of current dataframe for safety\n",
    "df_reg = df_clean.copy()\n",
    "\n",
    "# Define target or output or dependent variable\n",
    "target = 'erf_speich'\n",
    "\n",
    "# Define Regressors and Controls\n",
    "key_regressors = ['unsicher_high', 'burden_high', 'opportunity_high', 'si_speich']\n",
    "control_vars = ['pdwichtig', 'datengm', 'ds_unter_aktuell', 'ds_forsch_aktuell', 'kund_dat'] + list(size_dummies.columns) + list(sector_dummies.columns)\n",
    "cols = [target] + key_regressors + control_vars + ['aweight']\n",
    "\n",
    "for col in key_regressors + [target, 'ds_unter_aktuell', 'ds_forsch_aktuell'] + list(size_dummies.columns) + list(sector_dummies.columns):\n",
    "    df_reg[col] = df_reg[col].astype(int)  # or .astype(float)\n",
    "\n",
    "# Create X and y\n",
    "y = df_reg[target]\n",
    "X = df_reg[key_regressors + control_vars]\n",
    "X = sm.add_constant(X)\n",
    "weights = df_reg['aweight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "882368f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             erf_speich   R-squared:                       0.105\n",
      "Model:                            WLS   Adj. R-squared:                  0.064\n",
      "Method:                 Least Squares   F-statistic:                     1.816\n",
      "Date:                Sat, 21 Feb 2026   Prob (F-statistic):            0.00671\n",
      "Time:                        17:11:00   Log-Likelihood:                -447.45\n",
      "No. Observations:                 637   AIC:                             952.9\n",
      "Df Residuals:                     608   BIC:                             1082.\n",
      "Df Model:                          28                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      0.6226      0.154      4.045      0.000       0.321       0.924\n",
      "unsicher_high             -0.0378      0.060     -0.635      0.526      -0.155       0.079\n",
      "burden_high                0.0342      0.052      0.657      0.511      -0.068       0.136\n",
      "opportunity_high           0.0364      0.089      0.407      0.684      -0.139       0.212\n",
      "si_speich                  0.0747      0.025      3.039      0.002       0.027       0.123\n",
      "pdwichtig                 -0.0244      0.023     -1.056      0.291      -0.070       0.021\n",
      "datengm                   -0.0296      0.024     -1.235      0.217      -0.077       0.017\n",
      "ds_unter_aktuell          -0.0061      0.067     -0.092      0.927      -0.137       0.125\n",
      "ds_forsch_aktuell         -0.0053      0.114     -0.046      0.963      -0.229       0.219\n",
      "kund_dat                   0.0101      0.021      0.478      0.633      -0.031       0.052\n",
      "gk9_100-249                0.0286      0.068      0.420      0.674      -0.105       0.162\n",
      "gk9_20-49                  0.0648      0.062      1.043      0.297      -0.057       0.187\n",
      "gk9_250-499               -0.0237      0.097     -0.245      0.807      -0.214       0.166\n",
      "gk9_5-9                    0.0160      0.070      0.228      0.820      -0.122       0.154\n",
      "gk9_50-99                  0.0438      0.079      0.556      0.578      -0.111       0.198\n",
      "gk9_500-999               -0.0296      0.176     -0.168      0.867      -0.375       0.315\n",
      "gk9_>=1000                 0.0960      0.099      0.969      0.332      -0.098       0.290\n",
      "br08_Fahrzeugbau          -0.0099      0.096     -0.102      0.918      -0.199       0.179\n",
      "br08_FuE                  -0.0092      0.101     -0.091      0.928      -0.207       0.189\n",
      "br08_IKT-Dienstleister     0.1194      0.069      1.729      0.084      -0.016       0.255\n",
      "br08_IKT-Hardware          0.2369      0.072      3.288      0.001       0.096       0.378\n",
      "br08_Maschinenbau         -0.0420      0.089     -0.472      0.637      -0.216       0.132\n",
      "br08_Medienbranche         0.0161      0.090      0.178      0.859      -0.161       0.193\n",
      "br08_ReSteuWip             0.0040      0.138      0.029      0.977      -0.266       0.274\n",
      "br08_Sonstiges VG          0.0426      0.072      0.592      0.554      -0.098       0.184\n",
      "br08_UBeratung             0.0687      0.118      0.583      0.560      -0.162       0.300\n",
      "br08_Werbung              -0.0181      0.103     -0.175      0.861      -0.221       0.185\n",
      "br08_sonstigeDL           -0.0013      0.112     -0.011      0.991      -0.220       0.218\n",
      "br08_techDL                0.0579      0.083      0.701      0.483      -0.104       0.220\n",
      "==============================================================================\n",
      "Omnibus:                      309.366   Durbin-Watson:                   2.040\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1683.036\n",
      "Skew:                          -2.169   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.678   Cond. No.                         206.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "# Main model: weighted linear probability model (LPM) with robust SEs\n",
    "lpm_model = sm.WLS(y, X, weights=weights)\n",
    "lpm_results = lpm_model.fit(cov_type='HC1')  # heteroskedasticity-robust SEs\n",
    "print(lpm_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "916f5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             erf_speich   No. Observations:                  637\n",
      "Model:                            GLM   Df Residuals:                 80025.80\n",
      "Model Family:                Binomial   Df Model:                           27\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -34586.\n",
      "Date:                Sat, 21 Feb 2026   Deviance:                       69171.\n",
      "Time:                        17:04:14   Pearson chi2:                 8.21e+04\n",
      "No. Iterations:                     7   Pseudo R-squ. (CS):             0.9998\n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      2.2130      0.080     27.705      0.000       2.056       2.370\n",
      "unsicher_high             -0.6566      0.023    -29.042      0.000      -0.701      -0.612\n",
      "burden_high                0.2246      0.021     10.573      0.000       0.183       0.266\n",
      "opportunity_high           0.3884      0.097      4.002      0.000       0.198       0.579\n",
      "pdwichtig                 -0.1893      0.010    -18.041      0.000      -0.210      -0.169\n",
      "datengm                   -0.1750      0.012    -15.080      0.000      -0.198      -0.152\n",
      "ds_unter_aktuell          -0.1726      0.026     -6.560      0.000      -0.224      -0.121\n",
      "ds_forsch_aktuell          0.0787      0.041      1.925      0.054      -0.001       0.159\n",
      "kund_dat                   0.0805      0.010      8.339      0.000       0.062       0.099\n",
      "gk9_100-249                0.2787      0.044      6.390      0.000       0.193       0.364\n",
      "gk9_20-49                  0.4871      0.031     15.831      0.000       0.427       0.547\n",
      "gk9_250-499               -0.0837      0.065     -1.293      0.196      -0.211       0.043\n",
      "gk9_5-9                    0.0507      0.025      2.062      0.039       0.003       0.099\n",
      "gk9_50-99                  0.3072      0.038      8.009      0.000       0.232       0.382\n",
      "gk9_500-999               -0.0236      0.111     -0.212      0.832      -0.242       0.195\n",
      "gk9_>=1000                 1.5393      0.266      5.796      0.000       1.019       2.060\n",
      "br08_Fahrzeugbau          -0.0982      0.100     -0.987      0.324      -0.293       0.097\n",
      "br08_FuE                  -0.0366      0.097     -0.377      0.706      -0.227       0.154\n",
      "br08_IKT-Dienstleister     1.4957      0.078     19.185      0.000       1.343       1.649\n",
      "br08_IKT-Hardware          2.9767      0.271     10.990      0.000       2.446       3.508\n",
      "br08_Maschinenbau         -0.3121      0.075     -4.144      0.000      -0.460      -0.164\n",
      "br08_Medienbranche         0.1874      0.086      2.179      0.029       0.019       0.356\n",
      "br08_ReSteuWip             0.1327      0.075      1.774      0.076      -0.014       0.279\n",
      "br08_Sonstiges VG          0.1178      0.069      1.706      0.088      -0.018       0.253\n",
      "br08_UBeratung             0.4115      0.082      5.012      0.000       0.251       0.572\n",
      "br08_Werbung              -0.0680      0.083     -0.823      0.411      -0.230       0.094\n",
      "br08_sonstigeDL            0.1836      0.089      2.071      0.038       0.010       0.357\n",
      "br08_techDL                0.4263      0.075      5.693      0.000       0.280       0.573\n",
      "==========================================================================================\n",
      "         GLM Marginal Effects        \n",
      "=====================================\n",
      "Dep. Variable:             erf_speich\n",
      "Method:                          dydx\n",
      "At:                              mean\n",
      "==========================================================================================\n",
      "                            dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "unsicher_high             -0.0777      0.003    -27.232      0.000      -0.083      -0.072\n",
      "burden_high                0.0266      0.003     10.460      0.000       0.022       0.032\n",
      "opportunity_high           0.0459      0.011      4.005      0.000       0.023       0.068\n",
      "pdwichtig                 -0.0224      0.001    -17.575      0.000      -0.025      -0.020\n",
      "datengm                   -0.0207      0.001    -14.857      0.000      -0.023      -0.018\n",
      "ds_unter_aktuell          -0.0204      0.003     -6.538      0.000      -0.027      -0.014\n",
      "ds_forsch_aktuell          0.0093      0.005      1.926      0.054      -0.000       0.019\n",
      "kund_dat                   0.0095      0.001      8.285      0.000       0.007       0.012\n",
      "gk9_100-249                0.0330      0.005      6.546      0.000       0.023       0.043\n",
      "gk9_20-49                  0.0576      0.004     16.064      0.000       0.051       0.065\n",
      "gk9_250-499               -0.0099      0.008     -1.289      0.197      -0.025       0.005\n",
      "gk9_5-9                    0.0060      0.003      2.061      0.039       0.000       0.012\n",
      "gk9_50-99                  0.0363      0.005      8.067      0.000       0.028       0.045\n",
      "gk9_500-999               -0.0028      0.013     -0.212      0.832      -0.029       0.023\n",
      "gk9_>=1000                 0.1821      0.031      5.840      0.000       0.121       0.243\n",
      "br08_Fahrzeugbau          -0.0116      0.012     -0.987      0.323      -0.035       0.011\n",
      "br08_FuE                  -0.0043      0.011     -0.377      0.706      -0.027       0.018\n",
      "br08_IKT-Dienstleister     0.1769      0.010     17.762      0.000       0.157       0.196\n",
      "br08_IKT-Hardware          0.3521      0.029     12.025      0.000       0.295       0.409\n",
      "br08_Maschinenbau         -0.0369      0.009     -4.197      0.000      -0.054      -0.020\n",
      "br08_Medienbranche         0.0222      0.010      2.168      0.030       0.002       0.042\n",
      "br08_ReSteuWip             0.0157      0.009      1.763      0.078      -0.002       0.033\n",
      "br08_Sonstiges VG          0.0139      0.008      1.693      0.090      -0.002       0.030\n",
      "br08_UBeratung             0.0487      0.010      4.933      0.000       0.029       0.068\n",
      "br08_Werbung              -0.0080      0.010     -0.825      0.410      -0.027       0.011\n",
      "br08_sonstigeDL            0.0217      0.011      2.060      0.039       0.001       0.042\n",
      "br08_techDL                0.0504      0.009      5.576      0.000       0.033       0.068\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/statsmodels/genmod/generalized_linear_model.py:2381: UserWarning: weights are not taken into account by margeff\n",
      "  warnings.warn(\"weights are not taken into account by margeff\")\n"
     ]
    }
   ],
   "source": [
    "# Check model: weighted logit (GLM with binomial family)\n",
    "# Using GLM because it supports frequency / analytic weights more naturally\n",
    "logit_model = sm.GLM(\n",
    "    y,\n",
    "    X,\n",
    "    family=sm.families.Binomial(),\n",
    "    freq_weights=weights\n",
    ")\n",
    "logit_results = logit_model.fit()\n",
    "print(logit_results.summary())\n",
    "\n",
    "# 8. Optional: compute marginal effects for the logit\n",
    "marg_eff = logit_results.get_margeff(at='mean', method='dydx')\n",
    "print(marg_eff.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576d2d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
